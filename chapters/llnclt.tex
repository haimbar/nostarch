\runR{Code/llnclt_IQ.R}{llnclt_IQ}
\runR{Code/llnclt_roulette.R}{roulette}
\runR{Code/llnclt_texts.R}{texts}
\runR{Code/llnclt_Cauchy.R}{texts}
\runR{Code/llnclt_CLTexp.R}{cltsimexp}
\runR{Code/llnclt_CLTbinom.R}{cltsimbinom}
\runR{Code/llnclt_boygirl.R}{boygirl}
\runR{Code/llnclt_IQ2.R}{cltiq2}

\hypertarget{ch:llnclt}{%
%  \chapter{The Law of Large Numbers and the Central Limit Theorem}\label{ch:llnclt}}
\chapter{Laws of Large Samples: Where Chaos Becomes Order}\label{ch:llnclt}}
Benjamin Franklin once said `In this world nothing can be said to be certain,
except death and taxes'. Still, for all the uncertain things in life, and
especially in science where most theories cannot be proven correct, we would
like to be able to say something about how confident we are about a conclusion
we draw. To do that, scientists conduct an experiment in which they collect
data and check if it provides evidence in favor or against a proposed theory.
This chapter describes two of the most important results in probability and
statistics --- the law of large numbers and the central limit theorem. These
theorems explain why statistics works and is useful in so many situations! They
tell us that, if we collect enough data (and conduct a sensible experiment)
we'll be able to say to what extent we can trust our results, and also tell us
how much data we need in order to draw conclusions with a certain degree of
confidence. Without these results, conducting experiments would be meaningless,
no matter how many samples are collected.

\section{Introduction}
Let's start with a few simple examples which will be used in this chapter:
\begin{enumerate}
\item We get a random sample of 200 people and give them an IQ test. What is
the distribution of the scores in this sample? How close is it to the true
distribution of the whole population?\\ How likely is it that we find a person
with IQ greater than 150?\\
In order to be accepted to the Mensa club a person has to be in the top 2\% of
the IQ distribution. What is the minimum score required in order to be a Mensa
member?  \item The pockets of the roulette wheel are numbered from 0 to 36. In
number ranges from 1 to 10 and 19 to 28, odd numbers are red and even are
black. In ranges from 11 to 18 and 29 to 36, odd numbers are black and even are
red. There is a green pocket numbered 0 (zero). Suppose you want to bet on a
color, say, red, and if the ball lands in a red pocket your payoff is twice
your bet, and otherwise you lose your bet. What is the probability you win?
What is your expected payoff? Suppose you have \$10,000, and you bet one dollar
each time. How much money will you win or lose?.
\item Suppose that you get a text message every 6 minutes (a rate of 10 per
hour), and suppose that they arrive independently of one another, according to
some presumed distribution. How many messages do you expect to get on an
average day, between 8am and 8pm? 
%\item \hb{homework question?}As of June 16, 2021 Tony Snell from the Atlanta
%Hawks leads the NBA in 3pt accuracy with 53.8\%.
%If he takes 3pt 10 shots per game over the next 10 games, how many three-point shots do you expect him to make?
\end{enumerate}

The main paradigm in statistical inference and prediction in order to answer
such questions is to take a sample from the population we're interested in, and
choose a mathematical model which we believe represents the whole population.
Then we check if our model appears to be reasonable, in the sense that it fits the
data well. If it does, we can use the mathematical properties of the model to
draw conclusions about the population. By representing the sample using a
mathematical (probabilistic) formula, we essentially augment a finite sample to
an infinite one. Working with a formula/model for the data allows us to use the
`heavy artillery' of math, like finding maximum/minimum, computing areas under
the curve of the function, and so on. 

This chapter deals with inference which can be derived from the mean of a
population, and we show the (arguably) two most important results in statistics
-- the \emph{law of large numbers} (\emph{LLN}) and the \emph{central limit
theorem} (\emph{CLT}). Together, these two very general theorems make it
possible to draw conclusions about the whole population, from a single sample!
(as long as the sample represents the whole population and the sample
size is large enough).


\section{The Law of Large Numbers\index{Law of Large Numbers}}

\subsection{Example \#1 -- the IQ score}
%We get a random sample of 200 people and give them an IQ test. From this sample, we want to infer the true distribution in the entire population.

Suppose that the IQ in the general population has a normal distribution with
mean of 100 and  standard deviation of 15, and from this population we draw 200
people and calculate the sample mean and standard deviation, like so:

\showCode{R}{Code/llnclt_IQ.R}[1][4]
%\runR{Code/llnclt_IQ.R}{llnclt_IQ}

Run this code. You should get that the sample mean is
\inlnR{```cat(mean(samp1))```}[llnmsamp1] and the standard deviation is
\inlnR{```cat(sd(samp1))```}[llnssamp1]. Both are very close to the true values.

Let's pretend that just like in real life, we don't know the true distribution.
We propose a certain distribution and check if it fits. In the current
example, which we called \lstinline{samp1}, we want to check if the normal
distribution is a good model for our data.

\jy{QQ plot seems distracting here}
One way to check if a distribution fits our data is with a graphical tool
called the \emph{Q-Q plot}. A Q-Q plot is based on the simple (and very
important) idea that a distribution is determined by its \emph{quantiles}
(percentiles), 
so if the quantiles of our sample match the quantiles of the assumed
distribution, then the assumption is reasonable. Recall that we defined the
quantiles in Chapter~\ref{introdistributions}, so that the $q$-th quantile is a
number $x_q$, such that $q\times 100\%$ of the data are less than or equal to
$x_q$.  In our example the data has  \inlnR{```cat(n)```}[llnsampsize] data points, so we
can get \inlnR{```cat(n)```}[llnsampsize2] quantiles from our sample. If we draw the same
number of random numbers from the correct distribution, then their quantiles
will match the quantiles we got from our data (they will only match
approximately, because when we draw finite, random samples, there is always
some `noise'.)

When it's assumed that the data come from a normal distribution, we can use the
\lstinline{qqnorm()} function to check if the assumption is reasonable:
\showCode{R}{Code/llnclt_IQ.R}[7][8]

The points in the Q-Q plot in Figure~\ref{qqplot1} show the actual quantiles
(from the data, \lstinline{samp1}) versus the theoretical quantiles (those that
would be obtained if we drew a sample from a normal distribution). The points in
the graph lie very close to a straight line, indicating that the sample was
likely drawn from a normal distribution. (This is not surprising here because we
generated the sample this way. However, when we get a random sample and do not
know the true distribution, this plot is useful for checking whether our
mathematical model is reasonable.)

\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{images/chapter_4/qqplot1.pdf}
\caption{A Q-Q plot}
\label{qqplot1}
\end{figure}



To understand what the Law of Large Numbers says, let's repeat the previous
simulation but vary the sample size ($n$) and see what happens to the
\emph{sample mean} as we increase $n$.

\showCode{R}{Code/llnclt_IQ.R}[12][22]

\jy{how about putting sample size on log scale:
   \lstinline{ns <- rep(10 * 2^(0:11), each = 10)}'}

In \wingding{1} we create a vector of sample sizes. We will generate a random
sample for each of these \lstinline{ns}. In \wingding{2} we initialize a vector
which will hold all the sample means (at first, they are set to zero). The
\lstinline{for} loop in \wingding{3} is the same as the previous demonstration,
but in each iteration we increase the sample size, which is the first argument
of the function \lstinline{rnorm()}. The mean from the $i$-th sample is stored
in the $i$-th position of the vector \lstinline{allMeans} (in \wingding{4}.)

Figure~\ref{LLNdemo} shows the results. We can see that as $n$ increases, the
sample mean gets closer to the true mean (100, in our simulation), which is
shown as a horizontal line.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{images/chapter_4/LLNIQ.pdf}
\caption{Visualizing the Law of Large Numbers -- normal data}
\label{LLNdemo}
\end{figure}

We also see in Figure~\ref{LLNdemo} that the dispersion of the sample means
gets smaller as the sample size gets larger, but this is the result of the
\emph{Central Limit Theorem}, which will be discussed later in this chapter.

\subsection{Example \#2 -- the Roulette Wheel}
Here, the outcome is binary -- we either win or lose a bet. The probability of
winning if we choose the red color, is $18/37$.  Let's simulate 40 bets at the
roulette wheel, where, in each we put a dollar on red. Each bet is won with a
Bernoulli distribution, $Ber(18/37)$ and we repeat it 40 times. Equivalently,
we can look at it as a single binomial sample, with $n=40$ and $p=18/37$.

%\runR{Code/llnclt_roulette.R}{roulette}
\showCode{R}{Code/llnclt_roulette.R}[1][4]

Run this code. You should get in this particular sample of 40 bets, that the
probability of winning (the mean of \lstinline{samp2}) is
\inlnR{```cat(mean(samp2))```}[llnsamp2m] (greater than 0.5.) This seems great -- if we
keep going we might get rich! However, let's see what happens to the sample
mean (which is the probability of winning) as we increase $n$. The following
code is nearly identical to the one used in the previous example, except that
in this case we draw the random samples from a \emph{binomial} distribution (in
\wingding{1}.)

\showCode{R}{Code/llnclt_roulette.R}[7][17]

We see the results in Figure~\ref{LLNdemoBinomial}. As before, the sample mean
gets closer to the true mean of the distribution, $p=18/37$, which is
less than 0.5, so if we keep placing bets we will end up losing money. In fact,
we can check how much we will lose if we bet \$1 each time, and do it 10,000
times.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{images/chapter_4/LLNroulette.pdf}
\caption{Visualizing the Law of Large Numbers -- binomial data}
\label{LLNdemoBinomial}
\end{figure}

To simulate what will happen if we play the roulette table 10,000 times, each
time putting a dollar on red, run the following code:

\showCode{R}{Code/llnclt_roulette.R}[20][24]

The probability of winning in this simulation should be
\inlnR{```cat(mean(roulette))```}[llnmeanroulette]. We bet on \$10,000 and we won
\inlnR{```cat(sum(roulette))```}[llnsumroulette] times, so our winnings amounted to
\inlnR{```cat(prettyNum(2*sum(roulette), big.mark = ","))```}[llnprettysum] dollars, which
means that we lost  \inlnR{```cat(prettyNum(10000-2*sum(roulette), big.mark = ","))```}[llnprettysum2] dollars.

To the casino it doesn't matter if one person bets \$1,000,000 or if 1,000
people each bets on \$1,000 -- in both cases the casino will win with
probability 19/37 and will have a positive net gain (you can't beat the house).
Casinos know that, and use the law of large numbers in order to ensure that in
the long run, they will win.


\subsection{Example \#3 -- Receiving Text Messages}
Let's simulate $n$ days, and in each one count the number of messages, assuming
they arrive independently from a Poisson distribution with rate=10. To do that,
run the following code:

%\runR{Code/llnclt_texts.R}{texts}
\showCode{R}{Code/llnclt_texts.R}[2][10]

In each day we count messages over 12 hours, so in \wingding{1} we multiply the
(hourly) random sample from the Poisson distribution by 12. Figure
\ref{LLNdemoPoisson} shows the results.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{images/chapter_4/LLNtexts.pdf}
\caption{Visualizing the Law of Large Numbers -- Poisson data}
\label{LLNdemoPoisson}
\end{figure}

If we continue to get text messages at this fixed rate over a long period of
time, and we calculate the average daily messages (from 8am to 8pm) we see that
that average converges to 12*10=120 messages per day.



\subsection{The LLN\index{LLN} -- an Intuitive Statement of the Theorem}
Informally, the law of large numbers states that the results of an experiment
tend to settle down to a fixed average when the experiment is repeated many
times. In other words, as the sample size increases, the sample mean will get
closer to the true population mean.

The LLN is very general -- it doesn't matter if the question is about how many
boy-births we expect to see (\emph{binomial distribution}), the number of
received text messages (\emph{Poisson distibution}), or if we measure IQ
(\emph{normal distribution}). The only requirements are that the samples are
independent, and identically distributed, and that the true distribution
actually has a mean. (We will see an example where this is not the case.)

It's important to emphasize that the LLN doesn't imply that the results of
future experiments will balance out what already happened! If we bet 100 times
at the roulette wheel and lost each time, this doesn't mean that we have a
better chance of winning the next bet. All the LLN is saying is that if we
continue to play many times, the overall probability that we win in each round
will converge to 18/37.

\subsection{Example \#4 -- When the LLN\index{LLN} Doesn't Work}
We have two groups of students who have to take a math test. Group A has a mean
score of 70 and a standard deviation of 10, while group B has mean 60 and
standard deviation 20. Both groups have a normal distribution.  We take a
sample of size $n$ from each group and calculate the sample means. Then, we
calculate the difference between the sample means,
$d_{AB}=\bar{x}_A-\bar{x}_B$, and the ratio between the means,
$r_{AB}=\bar{x}_A/\bar{x}_B$.
As before, we increase the sample size and see how it affects $d_{AB}$ and $r_{AB}$.

%\runR{Code/llnclt_Cauchy.R}{texts}
\showCode{R}{Code/llnclt_Cauchy.R}[2][19]

In \wingding{1} and \wingding{2} we generate the samples from the two groups
(both follow a normal distribution), with the same sample size. In the \lstinline{for} loop
we vary the sample size to go from 50 to 5,000 in jumps of 10. In \wingding{3}
and \wingding{4} we calculate the difference and the ratio between the mean
scores in the two groups, respectively. In \wingding{5} and \wingding{6} we
keep the calculated values $d_{AB}$ and $r_{AB}$ in vectors called
\lstinline{allDiffs} and \lstinline{allRatios}, which are then used in the
\lstinline{plot()} function.


Figure~\ref{LLNdemoCauchy}, which is what you should get after running the
code,  shows the distributions of the difference between the average grades
($d_{AB}$) on the left, and of the ratio ($r_{AB}$) on the right.  What we see
here is that the mean difference between the groups, $d_{AB}$, converges to the
true difference between the groups (10), but the ratio, $r_{AB}$, doesn't
converge -- $n$ can be as large as we want, and we will still see ratios that
are quite extreme. (Note that the mean of a ratio between two distributions is
generally not equal to the ratio of the means of the distributions, so we don't
expect $r_{AB}$ to converge to exactly 70/60.)

\begin{figure}[H]
\includegraphics[width=1\textwidth]{images/chapter_4/LLNCauchy.pdf}
\caption{When the Law of Large Numbers\index{Law of Large Numbers} fails}
\label{LLNdemoCauchy}
\end{figure}

\jy{If the normals do not have mean zero, the ratio is not exactly
  Cauchy but Cauchy like in tails.}
What is happening here? The answer is that while a difference between two
normal random variables is still normal (and therefore, has a mean), the ratio
between two normal distributions follows a \emph{Cauchy distribution} which
doesn't have a theoretical mean, so the LLN doesn't apply to the ratios
between the grades of the two groups,  $r_{AB}$.

Although the LLN applies to almost any distribution we will ever encounter, the
lesson here is that taking the ratio between two perfectly well-behaved
distributions may lead to a distribution to which the LLN doesn't apply. Other
manipulations of random variables may lead to similar results.



\section{The Central Limit Theorem\index{Central Limit Theorem}}
The LLN says that as we increase the sample size, the sample mean converges to
a fixed value (the true population mean).  It doesn't say anything, however,
about how close the sample estimate is to the true value. In other words, it
says nothing about our level of (un)certainty. 

The central limit theorem (CLT\index{CLT}) says that, in most situations, if we could get
many random samples from the population, and calculate the mean from each
sample, and then plotted the many sample means as a histogram, then this
histogram will look like a normal distribution. This is true even if the
original variables themselves are not normally distributed!

The CLT requires very little about the actual distribution of the data: the
sample has to be independent and identically distributed (i.i.d.), and the
distribution must have a finite variance. 
\jy{We only defined sample variance but not population variance so far.}

\subsection{CLT -- Examples}
We will draw a random sample of size $n$ from the following distributions:
\begin{itemize}
%\item Standard normal (the \lstinline{rnorm(n,0,1)} function).
%\item Poisson with rate parameter $\lambda=3$ (use \lstinline{rpois(, 3)}).
\item Exponential with rate parameter 5.5 (using \lstinline{rexp(n, 5.5)}).
\item Binomial with probability 0.15.
\end{itemize}

In each case, we will plot a histogram of the sample, and overlay the
probability density function on top of it.  We will repeat it 10,000 times and
each time calculate the sample mean. Then, we will plot a histogram of the
10,000 sample means.

\subsubsection{Exponential Distribution}
First, let's draw one sample from Exp(5.5) with $n$=100 random i.i.d. draws,
and see that the distribution of the sample is very different from normal. In
the following code, we plot a histogram from one sample (with n=100) and
overlay the graph of the probability distribution function of Exp(5.5), in
order to show that the sample is representative of the true distribution:

\showCode{R}{Code/llnclt_CLTexp.R}[2][7]
%\runR{Code/llnclt_CLTexp.R}{cltsimexp}

Notice that the distribution in Figure~\ref{CLTexp} is very skewed, and doesn't
look at all like a normal distribution. Try running \lstinline{mean(expsamp1)}. you
should get \inlnR{```cat(mean(expsamp1))```}[llnmeansampexp], which is approximately
1/5.5, the theoretical mean of the Exp(5.5) distribution.

\jy{Add sample size and exponential paraemters to caption.}
\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{images/chapter_4/CLTexp.pdf}
\caption{Simulated data from an exponential distribution}
\label{CLTexp}
\end{figure}

To understand the CLT, we will draw 10,000 samples, calculate the mean of each
sample, and plot a histogram of those 10,000 sample means. We will first do it
with $n$=50, like so:

\showCode{R}{Code/llnclt_CLTexp.R}[12][22]

In \wingding{1} we prepare a vector which will hold the sample mean from 10,000
simulated datasets. In \wingding{2} we get the sample mean for the $i$th random
sample, and update the vector \lstinline{allMeans}. Then, we plot the histogram
and use \lstinline{abline} to show the true mean of the population (which is
1/5.5).

Repeat the same code, but change the first line to \lstinline{n <- 500}. Figure
\ref{CLTexp2} shows the results for $n=50$ (left) and $n=500$ (right).

\begin{figure}[H]
\includegraphics[width=0.9\textwidth]{images/chapter_4/CLTexp2.pdf}
\caption{The distribution of the sample mean from 10,000 simulated data from an
exponential distribution. Left: $n=50$, right: $n=500$} \label{CLTexp2}
\end{figure}

We can see that the mean is estimated quite accurately in both cases, and the
overall shape of the 10,000 sample means has the shape of a normal
distribution. However, the spread (variance) of the distribution decreases as
the sample size increases. This is the essence of the CLT!  It tells us that
the \emph{single} sample mean that we actually have is most likely not exactly
the true mean in the population, but if we can get a larger sample size ($n$)
then we'll know that the range in which we can expect to find the true mean is
getting smaller.

\subsubsection{Binomial Distribution}
In order to see the generality of the CLT, we demonstrate it again with
another, very different distribution. This time our data come from a discrete
and skewed distribution (the binomial).

The following code is very similar to the previous one, except that we use
\lstinline{rbinom} to generate the data. Try it, and then change the first line
to \lstinline{n <- 500}, and run it again.

\showCode{R}{Code/llnclt_CLTbinom.R}[3][13]
%\runR{Code/llnclt_CLTbinom.R}{cltsimbinom}

You should get the plot in Figure~\ref{CLTbinom}. Notice that because we've
simulated data with a relatively small probability of getting a success
($p$=0.15), when $n$=50 the shape of the distribution of sample means is still
a bit skewed, but when $n$ is large the bell shape appears!

\begin{figure}[H]
\includegraphics[width=0.9\textwidth]{images/chapter_4/CLTbinom.pdf}
\caption{The distribution of the sample mean from 10,000 simulated data from a
binomial distribution. Left: $n=50$, right: $n=500$} \label{CLTbinom}
\end{figure}

Once again, in both cases the mean of 10,000 simulated sample means is very
close to the true value (0.15), but as $n$ gets larger, we are more confident
that the \emph{only} value that we got from \emph{a single} experiment is close
to the true value, because the range in which we can find the true value is
narrower.

\subsubsection{Application to Child Births -- Boy/Girl Ratio}
In 2021 the boy to girl birth ratio in the USA is estimated to be 1.05. If we
took a random sample of 300 newborns across the USA, how many do you expect to
be boys? Run the following code:

\showCode{R}{Code/llnclt_boygirl.R}[1][8]
%\runR{Code/llnclt_boygirl.R}{boygirl}

This should give the following:
\includeOutput{boygirl}%[inline]

Would it be very surprising if we actually saw 148 in the random sample?
Suppose we took a sample of 3,000, instead. Would is be surprising if we
observed 1,480 boys?  How about if we took a sample of 30,000, instead. Would
is be surprising if we observed 14,800 boys?

The probability of a boy birth doesn't depend on the sample size, and we
calculated it to be 0.512, so the expected number of boys when the sample size
is 300, 3,000, and 30,000 is 151, 1,510, and 15,100, respectively. So, it may
appear that actually observing 148 instead of 151 is as likely as observing
1,480 instead of 1,510, or 14,800 instead of 15,100, but that is not true!  The
CLT tells us that when the sample size increases, the dispersion of the sample
means around the true mean gets smaller and smaller. This is demonstrated in
the code below. 
\showCode{R}{Code/llnclt_boygirl.R}[14][32]

The result is shown in Figure~\ref{CLTboygirl}.
The green vertical line represents the expected number of boys, and the red one
represents the observed ones. Notice that as we increase the sample size, the
likelihood of the observed number of boys (148, 1,480, 14,800) gets smaller.
When $n=30,000$ the red line is well outside the range of the simulated sample
means.

\begin{figure}[H]
\includegraphics[width=\textwidth]{images/chapter_4/CLTboygirl.pdf}
\caption{Number of boy births with different sample sizes ($n$=300, 3,000, and
30,000)} \label{CLTboygirl}
\end{figure}


\section{Application -- the  IQ Scores}
Let's revisit the IQ example, where we assumed that in the general population
IQ scores have a normal distribution with mean of 100 and  standard deviation
of 15.  We drew a sample of 200 people, and calculated the sample mean and
standard deviation, which turned out to be 99.4, and 15.6, respectively.

In the following code, we plot two curves on top of the histogram we obtain
from the sample -- one for the true normal distribution, and one using the
sample mean and sample standard deviation:
\showCode{R}{Code/llnclt_IQ2.R}[2][8]
%\runR{Code/llnclt_IQ2.R}{cltiq2}

Figure~\ref{CLTIQfit} shows the result. The red curve shows the true density
function, and the dashed green one shows the one estimated from the sample of
200 people. They are very close. The LLN tells us that as we increase our
sample size, the sample mean will be closer to the true mean.  The CLT tells us
how close to the true mean we can expect that our sample mean to be. As the
sample size gets larger, our confidence in how close we are to the true value
increases (because the range of the values we can get from random sampling
becomes narrower.)

\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{images/chapter_4/IQfit.pdf}
\caption{Distribution of 200 IQ scores}
\label{CLTIQfit}
\end{figure}


In order to answer the question how likely  it is to find a person with IQ
greater than 150, we can calculate the area of the right tail of the
distribution. We can do it with the following code:
\showCode{R}{Code/llnclt_IQ2.R}[11][11]
Try it. You should get that the probability to get a score of 150 or more is
only \inlnR{```cat(1-pnorm(150, mean=mean(samp), sd=sd(samp)))```}[llnIQtail]. A
very select group, indeed.

In the introduction to this chapter we also asked what has to be the minimum IQ
score required in order to be a Mensa member (the top 2\% in the IQ
distributions.) In the following code we use the \lstinline{quantile} function,
if we want to use the sample of 200 people and not make any assumption about
the shape of the distribution of IQ scores, or the \lstinline{qnorm} function
if we want to calculate the threshold based on the assumption that the scores
in the entire population follow a normal distribution. We can do it like so:
 
\showCode{R}{Code/llnclt_IQ2.R}[13][15]
Try it. You should get that the Mensa threshold is approximately
\inlnR{```cat(mensacutoff)```}[llnmensa].  We can show this threshold graphically.
In the following code we draw the density function of the normal distribution,
and fill the 2\% right tail of it 
\showCode{R}{Code/llnclt_IQ2.R}[18][23]

Figure~\ref{CLTIQMensa} shows the result. The shaded area covers exactly 2\% of
the area under the curve.

\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{images/chapter_4/IQtail.pdf}
\caption{The distribution of IQ's and the acceptance range to the Mensa club}
\label{CLTIQMensa}
\end{figure}


\section{Summary}
The LLN and CLT are very powerful results. Keep in mind that in real-life when
we do not know the true distribution, we can usually get just \textit{one
sample}, not 10,000 like we did in our simulations.  However, these theorems
tell us that as long as this one sample is large enough, we can get a very good
approximation for the distribution of the sample mean, \emph{as if we had
multiple samples}.
Not only that, but (almost) regardless of the distribution of the data, the sample mean
will have a normal distribution with mean which is equal to the true mean of
the original distribution (even if it's skewed, or discrete), and the variance
of the sample mean will shrink as we increase the sample size.  In the next
chapter we will see how the CLT can be used to test hypotheses and construct
confidence intervals for estimates. For example, we'll be able to answer
questions such as: ``You are given the IQ test results of 13 people and they
are: 139, 104, 115, 151, 116, 141, 117, 105, 134, 155, 130, 139, 121. Is this
group different than the overall population?''

%%% Local Variables:
%%% mode: latex
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-engine: xetex
%%% TeX-master: "../sidsmain.tex"
%%% End:
