\chapter{Estimation: A Hide-and-Seek Game}
\label{ch:estimation}

When data are collected from a population, the sample carries clues
about the broader population features that we want to
understand. These target quantities may be of characteristics of
interest, such as mean, median, selected quantiles, or standard
deviations of a variable, or measures describing how one variable
relates to another. Estimation is the process of using observed data
to infer these unknown population quantities. It is a hide-and-seek
game that we play with Mother Nature: the true features of the
data-generating process are unknown, and we attempt to uncover them
using the information available in our sample. It is essential to
remember that a single point estimate is never the whole story;
every estimate should be accompanied by a measure of uncertainty.


\section{Introduction}
\label{sec:est:intro}



Let us set up a simple hide-and-seek game in a concrete setting. 
Suppose a population is modeled by a normal distribution with an 
unknown location parameter $\mu$, which we may think of as the true 
average height of all 12-year-old boys in the United States. To keep the 
focus on the core ideas of estimation rather than algebraic details, we 
work with a simplified version of this model and assume the population 
variance is known; without loss of generality, we set $\sigma^2 = 1$. We 
observe a random sample of size $n$ and use it to learn about the hidden 
value of $\mu$.


The following code generates such a sample:

\showCode{R}{Code/est-hide.R}[1][5]

\runR{Code/est-hide.R}{hide}

The summary statistics for the observed sample are:
\inlnR{```print(summary(x))```}[estsummx][vbox]



An estimator of $\mu$ is required to be computable from the observed data and 
should not depend on any unknown quantities. Any number that can be computed 
solely from the sample is called a \emph{statistic}. \index{statistic} 
When a statistic is used to estimate a population quantity, it is called an 
\emph{estimator}. \index{estimator} Common examples include the 
\emph{mean} (the average of the sample values), the \emph{median} 
(the middle value when the data are sorted), the \emph{range} 
(the difference between the largest and smallest observations), and the 
\emph{midrange} (the midpoint between the minimum and maximum).  
With these tools in place, several natural questions arise:
\begin{enumerate}
\item What properties are desirable for an estimator?
\item When multiple estimators are available for the same target, how do we 
      assess which one is better?
\item How do we assess the uncertainty of an estimator?
\end{enumerate}


\section{Point Estimation: Seeking Strategies}

The location parameter of a normal distribution coincides with its mean and
median. The sample version of mean and median appear to be reasonably good
estimators. The sample mid-range, the average of the largest and smallest
observations of the sample range, also
seems a reasonable estimator for the population center. So, let us consider
three candidate estimators, or strategies to seek the truth:
\begin{itemize}
\item Sample mean
\item Sample median
\item Midrange
\end{itemize}


When an estimator is computed for a given sample, its value is called an
estimate\index{estimate}. That is, an estimator is a strategy and an estimate is
its realization given a dataset.


The three estimates based on the observed sample are obtained by:
\showCode{R}{Code/est-hide.R}[8][8]
Their values are \inlnR{```cat(estimates[1])```}[est1],
\inlnR{```cat(estimates[2])```}[est2], and \inlnR{```cat(estimates[3])```}[est3], respectively.


Which estimator is the best? In this game, the Nature knows the true
value of $\mu$, so it can compare the estimation errors across the three
estimates. Compared with the true $\mu$, the squared errors of the three
estimates are obtained by:
\showCode{R}{Code/est-hide.R}[13][13]
Their values are \inlnR{```cat(err[1])```}[esterr1],
\inlnR{```cat(err[2])```}[esterr3], and
\inlnR{```cat(err[3])```}[esterr3], respectively.


When we compare different methods of estimating the $\mu$, the comparison only
applies to the specific data sample we're looking at. A method that is usually
good might give worse results than a ``bad'' method just by chance for that
specific sample.  To assess the quality of an estimator, we can investigate its
properties; after all, an estimator is just a formula or rule that uses a random
sample to estimate something. Because the estimator depends on random data, it's
also random. Here are some properties about estimators:
\begin{enumerate}
\item If the average result of an estimator (over many random samples) is equal
  to the true value we're trying to estimate, then this estimator is
  \emph{unbiased}\index{estimator!unbiased}.  Mathematically, this is often
  written as $E(\hat\mu) = \mu$, where $E$ is the average $\hat\mu$ over all
  possible random samples.
\item The \emph{mean squared error (MSE)}\index{mean squared error} measures how
  far, on average, the estimator's result is from the true target. It combines
  both the error (or bias) and how spread out the estimates are. The formula is
  $E[(\hat\mu - \mu)^2]$.
\item An estimator with a smaller MSE than another estimator is more
  \emph{efficient}\index{estimator!efficient} than the other one, because it
  generally gives better results.
\end{enumerate}

Most of the theoretical properties of an estimator are studied asymptotically,
that is, assuming that the sample size goes to infinity. For finite sample sizes, their
properties are easily investigated via hide-and-seek games, or simulations. In
each simulation setting, we set up the hide-and-seek game and play the game
repeatedly for a large number of replicates. The bias and the MSE of an
estimator can be approximated by the sample averages from the simulations.


\subsection{Evaluating Estimators}

Now the game becomes a racing game with many repetitions. In each repetition, we
play hide-and-seek and obtain the squared errors of all candidate estimators.

\begin{example}[Hide-and-seek: Normal population]
\label{ex:hs:norm}
\runR{Code/est-hide-norm.R}{hide-norm}
We first set up the game where the data in each replication are generated from a
normal distribution with location $\mu$ and variance~1.
\showCode{R}{Code/est-hide-norm.R}[1][5]


The function \lstinline{do1rep} generates one sample of size \lstinline{n} from
the normal distribution with location parameter~$\mu$, collects three estimates
of $\mu$, and returns the squared errors of the three estimates in a vector.
Try it a few times to see what it does.
\showCode{R}{Code/est-hide-norm.R}[8][8]


Now we play the game repeatedly for 1000 times, each with sample
size \lstinline{n = 20} and see which estimator wins the
race on average.
\showCode{R}{Code/est-hide-norm.R}[11][14]
The results are
\inlnR{```print(rowMeans(sim))```}[estrowmeans1][vbox]
The first estimator, the sample mean, is a clear winner! The estimator (sample
median) follows and the third estimator (sample midrange) is the poorest among
the three.
\end{example}


Would the ordering be like this? Recall that the race here had data generated
from a normal distribution. One would guess that the true data generating scheme
may affect which estimator is the best or most efficient.


\begin{example}[Hide-and-seek: Cauchy population]
\runR{Code/est-hide-cauchy.R}{hide-cauchy}
Now let us generate data from a distribution, which has heavier tails than the
normal distribution. For a heavier-tailed distribution, we are more likely to
observe extreme-valued observations. Now it seems we can recycle
the \lstinline{do1rep} function and make it more general by taking a data
generation scheme as an input. 
\showCode{R}{Code/est-hide-cauchy.R}[1][5]
The argument \lstinline{datagen} is assumed to be a function with two inputs:
\lstinline{n} for sample size and \lstinline{mu} for a location parameter. When
\lstinline{datagen} is \lstinline{rnorm}, this function reduces to the version
in the last Example~\ref{ex:hs:norm}.


Now we can try the updated \lstinline{do1rep} function out with data from a
Cauchy distribution, which is the same as the $t$ distribution with~1 degree of
freedom.
\showCode{R}{Code/est-hide-cauchy.R}[8][8]
Then we run the race 1000 times again and check the MSE of three estimators from
the 1000 replicates.
\showCode{R}{Code/est-hide-cauchy.R}[13][15]
The results are
\inlnR{```print(rowMeans(sim.t))```}[estrowmeans2][vbox]
So, for this experiment, the second estimator, sample median, is the best among
the three.

The code has made it easy to compare the three estimators for data generated
from $t$ distributions with other degrees of freedom. One can verify that as the
degrees of freedom increases (so the $t$ distribution gets closer to the normal
distribution), the mean estimator eventually outperforms the median estimator.
\end{example}


The midrange estimator appears poor in both examples. This is intuitive since it
uses the least amount of information from the observed data, only the two
extreme observations. Would it ever be a good estimator in some situations?

\begin{example}[Hide-and-seek: Uniform population]
\runR{Code/est-hide-unif.R}{hide-unif}
The \lstinline{do1rep} is so general that we can easily run the comparison of
estimators in any data generation scheme. This time we generate data from a
uniform distribution centered at $\mu$ with a bandwidth of 20.
\showCode{R}{Code/est-hide-unif.R}
The results are
\inlnR{```print(rowMeans(sim.u))```}[estrowmeans3][vbox]
So, for this experiment, the third estimator, sample midrange, is the best among
the three. It uses the least amount of information, but what it uses contains
all the information about the location parameter $\mu$ in this setting.
\end{example}


It would be nice if one estimator is always the best regardless of what the true
data generating scheme is. Such estimator may not, however, exist.

\subsection{Maximum Likelihood Estimator}
To play the hide-and-seek game better, we need to select a good strategy. From
the examples in the last section, which strategy is better depends on the
underlying truth, which is unknown. There is, however, an estimator which is the
most efficient under quite general assumptions.


Let's start with a simple example.
\begin{example}[Likelihood in guessing jars]
\label{ex:has-jars}
There are 9 jars. The first jar has 9 red and 1 green balls; the second jar has
8 red and 2 green balls; $\cdots\cdots$; the 9th jar has 1 red and 9 green
balls. Now, one jar is randomly selected, from which one ball is randomly
selected, and the ball is red. Which jar was the one that was selected?


There are nine possibilities. If the selected jar were the first one, the
probability that a randomly selected ball is red is 9/10. If the selected jar
were the second one, the probability that a randomly selected ball is red is
8/10. Continue until the 9th jar. With this calculation, which jar should we
guess?


The first one!
\end{example}


The principle that we use here is the so-called \emph{maximum likelihood
estimation}\index{maximum likelihood estimation}. The \emph{parameter
space}\index{parameter space}, that is, the collection of all possible values of
the parameter, consists of only 9 jars. We choose the one that gives the highest
likelihood of observing a randomly selected ball being red.


For problems with continuous parameter spaces, the principle remains the
same. We compute the joint density of the observed sample as a function of the
parameters, and call this function the \emph{likelihood}\index{likelihood}
function. Then we maximize the likelihood with respect to the parameters. The
resulting maximizer is the \emph{maximum likelihood estimator (MLE)} of the
parameters. In R, a convenient function for finding MLE for univariate
distributions is \lstinline{fitdistr} in package \lstinline{MASS}.


\begin{example}[Maximum likelihood estimator]
\label{ex:mle}
Use \lstinline{MASS}
\end{example}


\section{Uncertainty}
Point estimation is not enough! We need some measure of uncertainty to make
inferences. This lead to interval estimators. The two endpoints of an interval
estimator are statistics such that the random interval formed by them captures
the unknown target parameter with certain probability. A
\emph{confidence interval}\index{confidence interval} with
\emph{confidence level}\index{confidence level} $1-\alpha$ means an interval
estimator of the target parameter such that the probability that the interval
covers the target is~$1-\alpha$.


\subsection{Large sample approximation}
One way to construct confidence intervals is to use the asymptotic
distributional properties of some estimator. In the simple situation of
estimating the population mean of a distribution, for example, we can use the
sample mean and the central limit theorem to construct an interval centered at
the sample mean. Suppose that we have a sample mean $\bar X_n$ based a sample of
size~$n$ from a population with mean $\mu$ and variance $\sigma^2$. From the
central limit theorem discussed in Chapter~\ref{ch:llnclt}, we know that $\bar
X_n$'s behavior can be approximated by a normal distribution with mean $\mu$ and
variance $\sigma^2/n$, if the sample size $n$ is large. Using this fact, we can
construct the following interval
\[
\left(\bar X_n - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \quad
\bar X_n + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\right),
\]
where $z_{\alpha/2}$ is the number so that the area between $-\alpha/2$ and
$\alpha/2$ under the standard normal density curve is $100(1-\alpha)\%$. This
is a confidence interval for $\mu$ with confidence level asymptotically $1 -
\alpha$. A 95\% confidence interval means $\alpha = 0.05$.

When the population variance $\sigma^2$ is unknown, we can use the sample
variance $S_n^2$ to substitute it, and the resulting confidence interval is
still asymptotically valid.


The \emph{coverage probability}\index{coverage probability} of a confidence
interval is the probability that the target is covered by the confidence
interval. Ideally, a confidence interval with confidence level~$1-\alpha$
should have coverage probability~$1-\alpha$. In practice, this may not be the
case, especially when the confidence intervals are constructed based on
asymptotic results but applied to finite sample sizes.


Now let's play a game to appreciate the importance of uncertainty in
estimation.
\begin{example}[Interval estimation for a gamma mean]
\label{ex:ci-clt}
\runR{Code/est-ci-clt.R}{ci-clt}[cache]
We write a function to construct a confidence interval with level $1 - \alpha$
for a population mean $\mu$ given an observed sample vector.
\showCode{R}{Code/est-ci-clt.R}[1][6]
Let's apply this to a have random of sample of size $n$ from a gamma
distribution with mean~$\mu$. For simplicity, let's set the the scale parameter
of the gamma distribution to be known as~$2$.
\showCode{R}{Code/est-ci-clt.R}[9][12]
The resulting 95\% confidence interval is
$($\inlnR{```cat(ci[1])```}[estci1], \inlnR{```cat(ci[2])```}[estci2]$)$.


Does a 95\% confidence interval constructed this way really give 95\%
probability of covering the truth? By theory, it should when the
sample size is large. Is $n = 50$ large enough for this to be good?
We can check the actual coverage rate in a simulation study.
\showCode{R}{Code/est-ci-clt.R}[15][22]
The empirical coverage rate from the 1000 replicates is
\inlnR{```cat(mean(sim50[1, ] < mu & sim50[2,] > mu))```}[estsim50].


What if the sample size is smaller, say 20?
\showCode{R}{Code/est-ci-clt.R}[26][27]
The empirical coverage now is
% the following causes a problem
\inlnR{```cat(mean(sim20[1, ] < mu & sim20[2,] > mu))```}[estsim20].


The simulation shows that the confidence intervals constructed from
the large sample theory works well for sample size $n = 50$ but not so
well for $n = 20$.
\end{example}

\subsection{Resampling}

Constructing confidence intervals depends on the uncertainty of the
estimator. Often times, a point estimator can be easily obtained, for example,
through optimizing an objective function or solving some estimating
equations. The variation of a point estimator, however, can be much more
difficult to obtain. Asymptotic approximations of the variance of an estimator
may be derived in some situations, but even in these situations, the asymptotic
variance approximation may not work well with small to moderate sample sizes.


\emph{Bootstrap}\index{bootstrap}
is a powerful tool to approximate the uncertainty of an estimator.
As a self-assisted approach, it approximates the variation of the sample by
treating the sample as a population and resample repeatedly from it with
replacement, each time forming a so-called bootstrap sample of the same size as
the original sample. For each bootstrap sample, we can obtain the point
estimator. The variation of the original point estimator can be approximated by
the empirical variation of a collection of the point estimators obtained from
the bootstrap samples.



Here we demonstrate how well bootstrap can approximate the variance of some
simple statistics. Suppose that \lstinline{statistic} is a function that takes a
random sample \lstinline{x} as input and returns a scalar statistic. A bootstrap
procedure to obtain a large number \lstinline{R} of bootstrap copies of the
statistic from a sample \lstinline{x} is implemented in the following function
\lstinline{myboot}.
\showCode{R}{Code/est-ci-boot.R}[1][10]


Function \lstinline{myboot} returns a list with two components. The first
component \lstinline{t0} is the scalar statistics computed from the observed
sample \lstinline{x}; the second component \lstinline{t} is a vector of dimension
\lstinline{R}, each element of which is a scalar statistic computed from a
bootstrap sample. We use the empirical variation of \lstinline{t} to approximate
the variation of \lstinline{t0}. An 95\% confidence interval can be constructed
simply by the 2.5 and 97.5 percentiles of \lstinline{t}. This type of confidence
interval is known as \emph{percentile bootstrap confidence
interval}\index{bootstrap!percentile confidence interval}.


\begin{example}[Bootstrap approximation of variance of sample median]
\runR{Code/est-ci-boot.R}{ci-boot}[cache]
Consider estimating the population median of a Gamma distribution with
shape 2 and scale 2 using the sample median. The population median can be
obtained from the \lstinline{qgamma()} function evaluated at 0.5 with the
specified parameters. Let's see how this works in action for a sample of
size~50.
\showCode{R}{Code/est-ci-boot.R}[14][20]

%% Tried putting cat(medBoot$t0) in math mode; not working
For this sample, our point estimate is
\inlnR{```cat(medBoot[["t0"]])```}[estmedboot], with 95\% confidence
interval \lstinline{medCI}, which is
$($\inlnR{```cat(medCI[1])```}[estmedci1], \inlnR{```cat(medCI[2])```}[estmedci2]$)$.
The true value \inlnR{```cat(target)```}[esttarget] is nicely captured by the interval
from this sample.


To investigate the coverage rate of the percentile bootstrap confidence
interval, we need to run a simulation study. Again, we layout what we need to do
in one replicate first:
\showCode{R}{Code/est-ci-boot.R}[23][27]


Function \lstinline{do1rep()} generates a random sample of size \lstinline{n}
from the $\Gamma(2, 2)$ distribution, performs a bootstrap procedure with 1000
bootstrap samples, and returns a vector of 3 elements: the point estimate and
the lower and upper bounds of a 95\% confidence interval. We replicate the
experiment \lstinline{nrep = 200} times and check the performance of the point
estimate and the empirical coverage rate of the 95\% confidence interval.
\showCode{R}{Code/est-ci-boot.R}[30][35]


The mean of the point estimate \lstinline{mean(sim50[1,])} from the
\inlnR{```cat(nrep)```}[estnrep1] replicates is
\inlnR{```cat(mean(sim50[1,]))```}[estmeansim50], which is very close to the true target
\inlnR{```cat(target)```}[esttarget2].


The empirical coverage rate of the 95\% confidence interval from the
\inlnR{```cat(nrep)```}[estnrep2] replicates is
\inlnR{```cat(mean(sim50[2,] < target & target < sim50[3,]))```}[estmeansim50ci],
which is reasonably close to the nominal level 95\%.


We repeat the process for a smaller sample size~20:
\showCode{R}{Code/est-ci-boot.R}[39][40]
The empirical coverage rate of the 95\% confidence interval from the
\inlnR{```cat(nrep)```}[estnrep3]  replicates is
%\inlnR{```cat(mean(sim20[2,] < target & target < sim20[3,]))```},
which is still reasonably close to the nominal level 95\%.
\end{example}



%%% Local Variables:
%%% mode: latex
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-engine: xetex
%%% TeX-master: "../sidsmain.tex"
%%% End:
